{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "2ab46b3f",
      "metadata": {
        "id": "2ab46b3f"
      },
      "source": [
        "# COVID‑19 Misinformation Detection – BERT Variants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "XD-_Y-6Jf92q",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XD-_Y-6Jf92q",
        "outputId": "2afc0bc9-b90a-49c6-e568-1a75d58951c4"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "from transformers import BertForSequenceClassification, BertTokenizer, BertConfig, BertModel\n",
        "from torch.optim import AdamW\n",
        "from sklearn.metrics import roc_auc_score, f1_score\n",
        "import time\n",
        "import datetime\n",
        "import re\n",
        "import random\n",
        "from string import punctuation\n",
        "from torch.utils.data import TensorDataset, random_split\n",
        "from torch.utils.data import DataLoader\n",
        "import gc\n",
        "\n",
        "# Set device\n",
        "torch.manual_seed(0)\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device('cuda' if use_cuda else 'cpu')\n",
        "if use_cuda:\n",
        "    torch.cuda.manual_seed(0)\n",
        "\n",
        "print(f\"Using GPU: {use_cuda}\")\n",
        "\n",
        "# Load and preprocess data\n",
        "train = pd.read_csv('Constraint_Train.csv')\n",
        "val = pd.read_csv('Constraint_Val.csv')\n",
        "train[\"label\"] = train[\"label\"].map({\"real\": 1, \"fake\": 0})\n",
        "val[\"label\"] = val[\"label\"].map({\"real\": 1, \"fake\": 0})\n",
        "\n",
        "# Combine datasets\n",
        "data = pd.concat([train, val], axis=0, ignore_index=True).drop([\"id\"], axis=1)\n",
        "\n",
        "# Initialize tokenizer\n",
        "print('Loading BERT tokenizer...')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "\n",
        "tweets = data.tweet.values\n",
        "labels = data.label.values\n",
        "\n",
        "# Text preprocessing function\n",
        "def preprocess(data):\n",
        "    # Remove URL and hashtag\n",
        "    for i in range(data.shape[0]):\n",
        "        text = data[i].lower()\n",
        "        text1 = ''.join([word+\" \" for word in text.split()])\n",
        "        data[i] = text1\n",
        "\n",
        "    # Regular expressions for cleaning\n",
        "    giant_url_regex = ('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|'\n",
        "        '[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
        "    mention_regex = '@[\\w\\-]+'\n",
        "    hashtag_regex = '#[\\w\\-]+'\n",
        "    space_pattern = '\\s+'\n",
        "\n",
        "    for i in range(data.shape[0]):\n",
        "        text_string = data[i]\n",
        "        parsed_text = re.sub(hashtag_regex, '', text_string)\n",
        "        parsed_text = re.sub(giant_url_regex, '', parsed_text)\n",
        "        parsed_text = re.sub(mention_regex, '', parsed_text)\n",
        "        # Remove punctuation\n",
        "        parsed_text = re.sub(r\"[{}]+\".format(punctuation), '', parsed_text)\n",
        "        parsed_text = re.sub(space_pattern, ' ', parsed_text)\n",
        "        data[i] = parsed_text\n",
        "    return data\n",
        "\n",
        "tweets = preprocess(tweets)\n",
        "print(\"Sample processed tweet:\", tweets[0])\n",
        "\n",
        "# Check the length distribution of tweets\n",
        "max_len = 0\n",
        "ind = [100, 200, 300, 400, 500, 512]\n",
        "for i in ind:\n",
        "    count = 0\n",
        "    for tweet in tweets:\n",
        "        max_len = max(max_len, len(tweet))\n",
        "        if len(tweet) > i:\n",
        "            count += 1\n",
        "    print(f\"Count of sentence length over {i} is: {count}\")\n",
        "print('Max sentence length: ', max_len)\n",
        "\n",
        "# Tokenize and encode data\n",
        "input_ids = []\n",
        "attention_masks = []\n",
        "for tweet in tweets:\n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "                        tweet,\n",
        "                        add_special_tokens = True,\n",
        "                        max_length = 512,\n",
        "                        truncation = True,\n",
        "                        padding = 'max_length',\n",
        "                        return_attention_mask = True,\n",
        "                        return_tensors = 'pt',\n",
        "                   )\n",
        "\n",
        "    input_ids.append(encoded_dict['input_ids'])\n",
        "    attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "# Convert lists to tensors\n",
        "input_ids = torch.cat(input_ids, dim=0)\n",
        "attention_masks = torch.cat(attention_masks, dim=0)\n",
        "labels = torch.tensor(labels)\n",
        "\n",
        "# Print sample\n",
        "print('Original: ', tweets[0])\n",
        "print('Token IDs:', input_ids[0])\n",
        "\n",
        "# Create dataset and split to train/validation\n",
        "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
        "train_size = int(0.9 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size], generator=torch.Generator().manual_seed(42))\n",
        "\n",
        "print(f'{train_size:>5,} training samples')\n",
        "print(f'{val_size:>5,} validation samples')\n",
        "\n",
        "# Create DataLoaders\n",
        "batch_size = 16\n",
        "train_dataloader = DataLoader(\n",
        "            train_dataset,\n",
        "            shuffle = True,\n",
        "            batch_size = batch_size\n",
        "        )\n",
        "\n",
        "validation_dataloader = DataLoader(\n",
        "            val_dataset,\n",
        "            shuffle = False,\n",
        "            batch_size = batch_size\n",
        "        )\n",
        "\n",
        "# Utility function for time formatting\n",
        "def format_time(elapsed):\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
        "\n",
        "# BERT Model Training Function (Unfrozen)\n",
        "def train_bert_unfrozen():\n",
        "    print(\"Training BERT unfrozen model...\")\n",
        "    # Initialize model\n",
        "    model = BertForSequenceClassification.from_pretrained(\n",
        "        \"bert-base-uncased\",\n",
        "        num_labels = 2,\n",
        "        output_attentions = False,\n",
        "        output_hidden_states = True,\n",
        "    )\n",
        "    model.to(device)\n",
        "\n",
        "    # Set parameters\n",
        "    optimizer = AdamW(model.parameters(),\n",
        "                    lr = 5e-5,\n",
        "                    eps = 1e-8\n",
        "                    )\n",
        "    epochs = 4\n",
        "\n",
        "    # Set seed\n",
        "    seed_val = 42\n",
        "    random.seed(seed_val)\n",
        "    torch.manual_seed(seed_val)\n",
        "    torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "    # Training loop\n",
        "    training_stats = []\n",
        "    total_t0 = time.time()\n",
        "    best_accuracy = 0\n",
        "    best_model_state = None\n",
        "\n",
        "    for epoch_i in range(0, epochs):\n",
        "        # Training\n",
        "        print(\"\")\n",
        "        print(f'Epoch {epoch_i + 1} / {epochs}')\n",
        "        print('Training...')\n",
        "\n",
        "        t0 = time.time()\n",
        "        total_train_loss = 0\n",
        "        total_train_accuracy = 0\n",
        "        model.train()\n",
        "\n",
        "        for step, batch in enumerate(train_dataloader):\n",
        "            input_ids = batch[0].to(device)\n",
        "            input_mask = batch[1].to(device)\n",
        "            labels = batch[2].to(device)\n",
        "\n",
        "            model.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(input_ids,\n",
        "                           token_type_ids=None,\n",
        "                           attention_mask=input_mask,\n",
        "                           labels=labels)\n",
        "\n",
        "            loss = outputs.loss\n",
        "            logits = outputs.logits\n",
        "\n",
        "            total_train_loss += loss.item()\n",
        "\n",
        "            # Backward pass\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "            # Calculate training accuracy\n",
        "            pred = torch.argmax(logits, dim=1)\n",
        "            total_train_accuracy += torch.sum(pred == labels).item()\n",
        "\n",
        "        # Calculate average metrics\n",
        "        avg_train_accuracy = total_train_accuracy / len(train_dataloader.dataset)\n",
        "        avg_train_loss = total_train_loss / len(train_dataloader.dataset)\n",
        "        print(f\"  Accuracy: {avg_train_accuracy}\")\n",
        "        print(f\"  Training loss: {avg_train_loss}\")\n",
        "\n",
        "        # Validation\n",
        "        print(\"\")\n",
        "        print(\"Validation...\")\n",
        "        model.eval()\n",
        "        total_eval_accuracy = 0\n",
        "        total_eval_loss = 0\n",
        "        y_true = []\n",
        "        y_pred = []\n",
        "\n",
        "        # No gradient calculation needed for validation\n",
        "        with torch.no_grad():\n",
        "            for batch in validation_dataloader:\n",
        "                input_ids = batch[0].to(device)\n",
        "                input_mask = batch[1].to(device)\n",
        "                labels = batch[2].to(device)\n",
        "\n",
        "                # Forward pass\n",
        "                outputs = model(input_ids,\n",
        "                               token_type_ids=None,\n",
        "                               attention_mask=input_mask,\n",
        "                               labels=labels)\n",
        "\n",
        "                loss = outputs.loss\n",
        "                logits = outputs.logits\n",
        "\n",
        "                total_eval_loss += loss.item()\n",
        "\n",
        "                # Calculate validation accuracy\n",
        "                pred = torch.argmax(logits, dim=1)\n",
        "                total_eval_accuracy += torch.sum(pred == labels).item()\n",
        "\n",
        "                # Store true labels and predictions for metrics\n",
        "                y_true.append(labels.cpu())\n",
        "                y_pred.append(pred.cpu())\n",
        "\n",
        "        # Calculate average metrics\n",
        "        avg_val_accuracy = total_eval_accuracy / len(validation_dataloader.dataset)\n",
        "        print(f\"  Accuracy: {avg_val_accuracy}\")\n",
        "        avg_val_loss = total_eval_loss / len(validation_dataloader.dataset)\n",
        "        print(f\"  Validation loss: {avg_val_loss}\")\n",
        "\n",
        "        training_time = format_time(time.time() - t0)\n",
        "        print()\n",
        "\n",
        "        # Concatenate batches\n",
        "        y_true = torch.cat(y_true).numpy()\n",
        "        y_pred = torch.cat(y_pred).numpy()\n",
        "\n",
        "        # Calculate metrics\n",
        "        print(f\"This epoch took: {training_time}\")\n",
        "        roc_auc = roc_auc_score(y_true, y_pred)\n",
        "        f1 = f1_score(y_true, y_pred)\n",
        "        print(f'ROC-AUC score: {roc_auc}')\n",
        "        print(f'F1 score: {f1}')\n",
        "        print()\n",
        "\n",
        "        # Store stats\n",
        "        training_stats.append(\n",
        "            {\n",
        "                'epoch': epoch_i + 1,\n",
        "                'Train Accur.': avg_train_accuracy,\n",
        "                'Training Loss': avg_train_loss,\n",
        "                'Valid. Loss': avg_val_loss,\n",
        "                'Valid. Accur.': avg_val_accuracy,\n",
        "                'ROC-AUC': roc_auc,\n",
        "                'F1': f1,\n",
        "                'Training Time': training_time,\n",
        "            }\n",
        "        )\n",
        "\n",
        "        # Save best model\n",
        "        if avg_val_accuracy > best_accuracy:\n",
        "            best_accuracy = avg_val_accuracy\n",
        "            best_model_state = model.state_dict().copy()\n",
        "\n",
        "    print()\n",
        "    print(\"=\"*10)\n",
        "    print(\"Summary\")\n",
        "    print(f\"Total time {format_time(time.time()-total_t0)}\")\n",
        "    print(f\"Best validation accuracy: {best_accuracy}\")\n",
        "\n",
        "    # Create a new model instance and load the best saved weights\n",
        "    final_model = BertForSequenceClassification.from_pretrained(\n",
        "        \"bert-base-uncased\",\n",
        "        num_labels = 2,\n",
        "        output_attentions = False,\n",
        "        output_hidden_states = True,\n",
        "    )\n",
        "    final_model.load_state_dict(best_model_state)\n",
        "    final_model.to(device)\n",
        "\n",
        "    # Save the model state instead of the entire model\n",
        "    torch.save(final_model.state_dict(), \"bert_unfrozen.pt\")\n",
        "\n",
        "    return final_model, training_stats\n",
        "\n",
        "# Custom BERT model with frozen encoder\n",
        "class BertFrozenEncoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BertFrozenEncoder, self).__init__()\n",
        "        # Load base BERT model\n",
        "        self.bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "        # Freeze all BERT parameters\n",
        "        for param in self.bert.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        # Add classifier layer\n",
        "        self.classifier = nn.Linear(768, 2)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None, token_type_ids=None, labels=None):\n",
        "        # Get BERT outputs\n",
        "        outputs = self.bert(input_ids=input_ids,\n",
        "                           attention_mask=attention_mask,\n",
        "                           token_type_ids=token_type_ids)\n",
        "\n",
        "        # Get CLS token representation (for classification)\n",
        "        pooled_output = outputs.pooler_output\n",
        "\n",
        "        # Get logits\n",
        "        logits = self.classifier(pooled_output)\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            loss_fct = nn.CrossEntropyLoss()\n",
        "            loss = loss_fct(logits, labels)\n",
        "\n",
        "        # Return outputs similar to BertForSequenceClassification\n",
        "        return type('BertOutput', (), {\n",
        "            'loss': loss,\n",
        "            'logits': logits,\n",
        "            'hidden_states': outputs.hidden_states,\n",
        "            'attentions': outputs.attentions\n",
        "        })\n",
        "\n",
        "# BERT Model Training Function (Frozen)\n",
        "def train_bert_frozen():\n",
        "    print(\"Training BERT frozen model...\")\n",
        "    # Initialize model\n",
        "    model = BertFrozenEncoder()\n",
        "    model.to(device)\n",
        "\n",
        "    # Set parameters\n",
        "    optimizer = AdamW(model.classifier.parameters(),  # Only train classifier\n",
        "                    lr = 5e-5,\n",
        "                    eps = 1e-8\n",
        "                    )\n",
        "    epochs = 4\n",
        "\n",
        "    # Set seed\n",
        "    seed_val = 42\n",
        "    random.seed(seed_val)\n",
        "    torch.manual_seed(seed_val)\n",
        "    torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "    # Training loop\n",
        "    training_stats = []\n",
        "    total_t0 = time.time()\n",
        "    best_accuracy = 0\n",
        "    best_model_state = None\n",
        "\n",
        "    for epoch_i in range(0, epochs):\n",
        "        # Training\n",
        "        print(\"\")\n",
        "        print(f'Epoch {epoch_i + 1} / {epochs}')\n",
        "        print('Training...')\n",
        "\n",
        "        t0 = time.time()\n",
        "        total_train_loss = 0\n",
        "        total_train_accuracy = 0\n",
        "        model.train()\n",
        "\n",
        "        for step, batch in enumerate(train_dataloader):\n",
        "            input_ids = batch[0].to(device)\n",
        "            input_mask = batch[1].to(device)\n",
        "            labels = batch[2].to(device)\n",
        "\n",
        "            model.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(input_ids,\n",
        "                           token_type_ids=None,\n",
        "                           attention_mask=input_mask,\n",
        "                           labels=labels)\n",
        "\n",
        "            loss = outputs.loss\n",
        "            logits = outputs.logits\n",
        "\n",
        "            total_train_loss += loss.item()\n",
        "\n",
        "            # Backward pass\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.classifier.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "            # Calculate training accuracy\n",
        "            pred = torch.argmax(logits, dim=1)\n",
        "            total_train_accuracy += torch.sum(pred == labels).item()\n",
        "\n",
        "        # Calculate average metrics\n",
        "        avg_train_accuracy = total_train_accuracy / len(train_dataloader.dataset)\n",
        "        avg_train_loss = total_train_loss / len(train_dataloader.dataset)\n",
        "        print(f\"  Accuracy: {avg_train_accuracy}\")\n",
        "        print(f\"  Training loss: {avg_train_loss}\")\n",
        "\n",
        "        # Validation\n",
        "        print(\"\")\n",
        "        print(\"Validation...\")\n",
        "        model.eval()\n",
        "        total_eval_accuracy = 0\n",
        "        total_eval_loss = 0\n",
        "        y_true = []\n",
        "        y_pred = []\n",
        "\n",
        "        # No gradient calculation needed for validation\n",
        "        with torch.no_grad():\n",
        "            for batch in validation_dataloader:\n",
        "                input_ids = batch[0].to(device)\n",
        "                input_mask = batch[1].to(device)\n",
        "                labels = batch[2].to(device)\n",
        "\n",
        "                # Forward pass\n",
        "                outputs = model(input_ids,\n",
        "                               token_type_ids=None,\n",
        "                               attention_mask=input_mask,\n",
        "                               labels=labels)\n",
        "\n",
        "                loss = outputs.loss\n",
        "                logits = outputs.logits\n",
        "\n",
        "                total_eval_loss += loss.item()\n",
        "\n",
        "                # Calculate validation accuracy\n",
        "                pred = torch.argmax(logits, dim=1)\n",
        "                total_eval_accuracy += torch.sum(pred == labels).item()\n",
        "\n",
        "                # Store true labels and predictions for metrics\n",
        "                y_true.append(labels.cpu())\n",
        "                y_pred.append(pred.cpu())\n",
        "\n",
        "        # Calculate average metrics\n",
        "        avg_val_accuracy = total_eval_accuracy / len(validation_dataloader.dataset)\n",
        "        print(f\"  Accuracy: {avg_val_accuracy}\")\n",
        "        avg_val_loss = total_eval_loss / len(validation_dataloader.dataset)\n",
        "        print(f\"  Validation loss: {avg_val_loss}\")\n",
        "\n",
        "        training_time = format_time(time.time() - t0)\n",
        "        print()\n",
        "\n",
        "        # Concatenate batches\n",
        "        y_true = torch.cat(y_true).numpy()\n",
        "        y_pred = torch.cat(y_pred).numpy()\n",
        "\n",
        "        # Calculate metrics\n",
        "        print(f\"This epoch took: {training_time}\")\n",
        "        roc_auc = roc_auc_score(y_true, y_pred)\n",
        "        f1 = f1_score(y_true, y_pred)\n",
        "        print(f'ROC-AUC score: {roc_auc}')\n",
        "        print(f'F1 score: {f1}')\n",
        "        print()\n",
        "\n",
        "        # Store stats\n",
        "        training_stats.append(\n",
        "            {\n",
        "                'epoch': epoch_i + 1,\n",
        "                'Train Accur.': avg_train_accuracy,\n",
        "                'Training Loss': avg_train_loss,\n",
        "                'Valid. Loss': avg_val_loss,\n",
        "                'Valid. Accur.': avg_val_accuracy,\n",
        "                'ROC-AUC': roc_auc,\n",
        "                'F1': f1,\n",
        "                'Training Time': training_time,\n",
        "            }\n",
        "        )\n",
        "\n",
        "        # Save best model\n",
        "        if avg_val_accuracy > best_accuracy:\n",
        "            best_accuracy = avg_val_accuracy\n",
        "            best_model_state = model.state_dict().copy()\n",
        "\n",
        "    print()\n",
        "    print(\"=\"*10)\n",
        "    print(\"Summary\")\n",
        "    print(f\"Total time {format_time(time.time()-total_t0)}\")\n",
        "    print(f\"Best validation accuracy: {best_accuracy}\")\n",
        "\n",
        "    # Create a new model instance and load the best saved weights\n",
        "    final_model = BertFrozenEncoder()\n",
        "    final_model.load_state_dict(best_model_state)\n",
        "    final_model.to(device)\n",
        "\n",
        "    # Save the model state instead of the entire model\n",
        "    torch.save(final_model.state_dict(), \"bert_frozen.pt\")\n",
        "\n",
        "    return final_model, training_stats\n",
        "\n",
        "# Main execution flow\n",
        "def main():\n",
        "    # Train unfrozen BERT model\n",
        "    print(\"Starting training for unfrozen BERT model...\")\n",
        "    unfrozen_model, unfrozen_stats = train_bert_unfrozen()\n",
        "\n",
        "    # Free up GPU memory\n",
        "    del unfrozen_model\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "    # Train frozen BERT model\n",
        "    print(\"Starting training for frozen BERT model...\")\n",
        "    frozen_model, frozen_stats = train_bert_frozen()\n",
        "\n",
        "    # Compare performance metrics\n",
        "    print(\"\\nPerformance comparison:\")\n",
        "    print(\"\\nUnfrozen BERT:\")\n",
        "    for epoch, stat in enumerate(unfrozen_stats):\n",
        "        print(f\"Epoch {epoch+1}: Accuracy = {stat['Valid. Accur.']:.4f}, F1 = {stat['F1']:.4f}\")\n",
        "\n",
        "    print(\"\\nFrozen BERT:\")\n",
        "    for epoch, stat in enumerate(frozen_stats):\n",
        "        print(f\"Epoch {epoch+1}: Accuracy = {stat['Valid. Accur.']:.4f}, F1 = {stat['F1']:.4f}\")\n",
        "\n",
        "    # Plot training and validation loss for both models\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    # Plot training loss\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot([stat['Training Loss'] for stat in frozen_stats], label='Frozen BERT')\n",
        "    plt.plot([stat['Training Loss'] for stat in unfrozen_stats], label='Unfrozen BERT')\n",
        "    plt.title('Training Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    # Plot validation loss\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot([stat['Valid. Loss'] for stat in frozen_stats], label='Frozen BERT')\n",
        "    plt.plot([stat['Valid. Loss'] for stat in unfrozen_stats], label='Unfrozen BERT')\n",
        "    plt.title('Validation Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('loss_comparison.png')\n",
        "    plt.close()\n",
        "\n",
        "    # Plot training and validation accuracy for both models\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    # Plot training accuracy\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot([stat['Train Accur.'] for stat in frozen_stats], label='Frozen BERT')\n",
        "    plt.plot([stat['Train Accur.'] for stat in unfrozen_stats], label='Unfrozen BERT')\n",
        "    plt.title('Training Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    # Plot validation accuracy\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot([stat['Valid. Accur.'] for stat in frozen_stats], label='Frozen BERT')\n",
        "    plt.plot([stat['Valid. Accur.'] for stat in unfrozen_stats], label='Unfrozen BERT')\n",
        "    plt.title('Validation Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('accuracy_comparison.png')\n",
        "    plt.close()\n",
        "\n",
        "    print(\"\\nTraining complete. Model states saved to 'bert_frozen.pt' and 'bert_unfrozen.pt'\")\n",
        "    print(\"Performance comparison plots saved to 'loss_comparison.png' and 'accuracy_comparison.png'\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Install required packages if running on Colab\n",
        "    try:\n",
        "        import google.colab\n",
        "        print(\"Running on Colab, installing required packages...\")\n",
        "        !pip install -q transformers matplotlib\n",
        "    except:\n",
        "        print(\"Not running on Colab\")\n",
        "\n",
        "    # Run the main function\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6fc27e86",
      "metadata": {},
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "o6Vc7vk_msT9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o6Vc7vk_msT9",
        "outputId": "7e6a68e3-7abd-4cb5-ca49-f7db0830b35a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using GPU: True\n",
            "Loading BERT tokenizer...\n",
            "Sample processed tweet: the cdc currently reports 99031 deaths in general the discrepancies in death counts between different sources are small and explicable the death toll stands at roughly 100000 people today \n",
            "7,704 training samples\n",
            "  856 validation samples\n",
            "Running on Colab, installing required packages...\n",
            "Loading trained models...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded unfrozen model state_dict successfully\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "BertSdpaSelfAttention is used but `torch.nn.functional.scaled_dot_product_attention` does not support non-absolute `position_embedding_type` or `output_attentions=True` or `head_mask`. Falling back to the manual attention implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded frozen model state_dict successfully\n",
            "\n",
            "Extracting feature importance for unfrozen model...\n",
            "Top 30 important words for unfrozen model:\n",
            "trump: 0.0529\n",
            "##s: 0.0458\n",
            "faint: 0.0457\n",
            "[CLS]: 0.0422\n",
            "is: 0.0409\n",
            "infected: 0.0405\n",
            "[PAD]: 0.0400\n",
            "with: 0.0369\n",
            "[SEP]: 0.0346\n",
            "corona: 0.0332\n",
            "##virus: 0.0318\n",
            "\n",
            "Extracting feature importance for frozen model...\n",
            "Top 30 important words for frozen model:\n",
            "[CLS]: 0.0827\n",
            "trump: 0.0454\n",
            "[PAD]: 0.0410\n",
            "faint: 0.0395\n",
            "##s: 0.0379\n",
            "is: 0.0356\n",
            "infected: 0.0347\n",
            "[SEP]: 0.0329\n",
            "with: 0.0315\n",
            "corona: 0.0285\n",
            "##virus: 0.0270\n",
            "\n",
            "Analyzing prediction confidence for unfrozen model...\n",
            "Average confidence: 0.9977\n",
            "Average confidence for correct predictions: 0.9985\n",
            "Average confidence for incorrect predictions: 0.9835\n",
            "\n",
            "Analyzing prediction confidence for frozen model...\n",
            "Average confidence: 0.6510\n",
            "Average confidence for correct predictions: 0.6665\n",
            "Average confidence for incorrect predictions: 0.5974\n",
            "Analyzing unfrozen model errors...\n",
            "\n",
            "Analyzing errors for bert_unfrozen model...\n",
            "Total examples: 856\n",
            "Correct predictions: 811 (94.74%)\n",
            "Incorrect predictions: 45 (5.26%)\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Fake       0.97      0.92      0.94       393\n",
            "        Real       0.93      0.97      0.95       463\n",
            "\n",
            "    accuracy                           0.95       856\n",
            "   macro avg       0.95      0.95      0.95       856\n",
            "weighted avg       0.95      0.95      0.95       856\n",
            "\n",
            "\n",
            "Real news classified as fake - avg length: 19.25 words\n",
            "Fake news classified as real - avg length: 23.48 words\n",
            "\n",
            "Most common words in real news classified as fake:\n",
            "[('a', 8), ('is', 6), ('and', 5), ('with', 5), ('covid19', 4), ('the', 4), ('how', 3), ('’', 3), ('coronavirus', 3), ('to', 3), ('•', 3), ('or', 3), ('vaccine', 3), ('only', 3), ('at', 3), ('in', 3), ('it', 2), ('for', 2), ('disease', 2), ('by', 2)]\n",
            "\n",
            "Most common words in fake news classified as real:\n",
            "[('the', 43), ('of', 26), ('to', 18), ('covid19', 17), ('is', 13), ('and', 12), ('in', 11), ('a', 11), ('with', 11), ('for', 8), ('that', 8), ('not', 5), ('health', 5), ('we', 5), ('all', 5), ('new', 5), ('cases', 4), ('as', 4), ('state', 4), ('’', 4)]\n",
            "\n",
            "Avg confidence when real news classified as fake: 0.9913\n",
            "Avg confidence when fake news classified as real: 0.9807\n",
            "Analyzing frozen model errors...\n",
            "\n",
            "Analyzing errors for bert_frozen model...\n",
            "Total examples: 856\n",
            "Correct predictions: 664 (77.57%)\n",
            "Incorrect predictions: 192 (22.43%)\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Fake       0.74      0.78      0.76       393\n",
            "        Real       0.80      0.77      0.79       463\n",
            "\n",
            "    accuracy                           0.78       856\n",
            "   macro avg       0.77      0.78      0.77       856\n",
            "weighted avg       0.78      0.78      0.78       856\n",
            "\n",
            "\n",
            "Real news classified as fake - avg length: 22.58 words\n",
            "Fake news classified as real - avg length: 29.36 words\n",
            "\n",
            "Most common words in real news classified as fake:\n",
            "[('the', 115), ('to', 69), ('of', 61), ('in', 55), ('and', 51), ('a', 46), ('for', 32), ('is', 24), ('more', 24), ('are', 23), ('at', 19), ('from', 18), ('’', 17), ('india', 16), ('have', 15), ('on', 15), ('be', 14), ('has', 14), ('cases', 13), ('coronavirus', 13)]\n",
            "\n",
            "Most common words in fake news classified as real:\n",
            "[('the', 108), ('of', 67), ('to', 67), ('and', 57), ('a', 45), ('covid19', 41), ('in', 38), ('is', 36), ('for', 30), ('that', 27), ('was', 24), ('this', 22), ('from', 21), ('coronavirus', 19), ('with', 18), ('it', 18), ('not', 17), ('all', 16), ('by', 15), ('be', 15)]\n",
            "\n",
            "Avg confidence when real news classified as fake: 0.6083\n",
            "Avg confidence when fake news classified as real: 0.5842\n",
            "Comparing models...\n",
            "\n",
            "Model Comparison - Frozen vs. Unfrozen BERT:\n",
            "Agreement rate: 0.7769 (665/856)\n",
            "Both correct: 642/856 (75.00%)\n",
            "Both wrong: 23/856 (2.69%)\n",
            "Only frozen correct: 22/856 (2.57%)\n",
            "Only unfrozen correct: 169/856 (19.74%)\n",
            "\n",
            "Frozen model accuracy: 0.7757\n",
            "Unfrozen model accuracy: 0.9474\n",
            "Frozen model F1 score: 0.7885\n",
            "Unfrozen model F1 score: 0.9525\n",
            "\n",
            "Examples where models disagree:\n",
            "\n",
            "Example 1:\n",
            "Text: jeanine anez actual president of bolivia has covid19...\n",
            "True label: Fake\n",
            "Frozen BERT prediction: Real\n",
            "Unfrozen BERT prediction: Fake\n",
            "Correct model: Unfrozen\n",
            "\n",
            "Example 2:\n",
            "Text: shadow cabinet office minister tells she thinks the govts suggestion of using the army to support po...\n",
            "True label: Real\n",
            "Frozen BERT prediction: Fake\n",
            "Unfrozen BERT prediction: Real\n",
            "Correct model: Unfrozen\n",
            "\n",
            "Example 3:\n",
            "Text: india scales another peak in last 24 hours 12 lakh tests were conducted across the country which is ...\n",
            "True label: Real\n",
            "Frozen BERT prediction: Fake\n",
            "Unfrozen BERT prediction: Real\n",
            "Correct model: Unfrozen\n",
            "\n",
            "Example 4:\n",
            "Text: breaking sky news understands further lockdown restrictions are set to be imposed in the north east ...\n",
            "True label: Real\n",
            "Frozen BERT prediction: Fake\n",
            "Unfrozen BERT prediction: Real\n",
            "Correct model: Unfrozen\n",
            "\n",
            "Example 5:\n",
            "Text: the number of confirmed cases is no longer falling in any region of the country the northeast has be...\n",
            "True label: Real\n",
            "Frozen BERT prediction: Fake\n",
            "Unfrozen BERT prediction: Real\n",
            "Correct model: Unfrozen\n",
            "\n",
            "Analysis complete. Results saved to files.\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "from transformers import BertForSequenceClassification, BertTokenizer, BertConfig, BertModel\n",
        "from sklearn.metrics import roc_auc_score, f1_score, confusion_matrix, classification_report\n",
        "import time\n",
        "import datetime\n",
        "import re\n",
        "import random\n",
        "import seaborn as sns\n",
        "from string import punctuation\n",
        "from torch.utils.data import TensorDataset, random_split\n",
        "from torch.utils.data import DataLoader\n",
        "from collections import Counter\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "import gc\n",
        "\n",
        "# Set device\n",
        "torch.manual_seed(0)\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device('cuda' if use_cuda else 'cpu')\n",
        "if use_cuda:\n",
        "    torch.cuda.manual_seed(0)\n",
        "\n",
        "print(f\"Using GPU: {use_cuda}\")\n",
        "\n",
        "# Load and preprocess data\n",
        "train = pd.read_csv('Constraint_Train.csv')\n",
        "val = pd.read_csv('Constraint_Val.csv')\n",
        "train[\"label\"] = train[\"label\"].map({\"real\": 1, \"fake\": 0})\n",
        "val[\"label\"] = val[\"label\"].map({\"real\": 1, \"fake\": 0})\n",
        "\n",
        "# Combine datasets\n",
        "data = pd.concat([train, val], axis=0, ignore_index=True).drop([\"id\"], axis=1)\n",
        "\n",
        "# Initialize tokenizer\n",
        "print('Loading BERT tokenizer...')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "\n",
        "tweets = data.tweet.values\n",
        "labels = data.label.values\n",
        "\n",
        "# Text preprocessing function\n",
        "def preprocess(data):\n",
        "    # Remove URL and hashtag\n",
        "    for i in range(data.shape[0]):\n",
        "        text = data[i].lower()\n",
        "        text1 = ''.join([word+\" \" for word in text.split()])\n",
        "        data[i] = text1\n",
        "\n",
        "    # Regular expressions for cleaning\n",
        "    giant_url_regex = ('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|'\n",
        "        '[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
        "    mention_regex = '@[\\w\\-]+'\n",
        "    hashtag_regex = '#[\\w\\-]+'\n",
        "    space_pattern = '\\s+'\n",
        "\n",
        "    for i in range(data.shape[0]):\n",
        "        text_string = data[i]\n",
        "        parsed_text = re.sub(hashtag_regex, '', text_string)\n",
        "        parsed_text = re.sub(giant_url_regex, '', parsed_text)\n",
        "        parsed_text = re.sub(mention_regex, '', parsed_text)\n",
        "        # Remove punctuation\n",
        "        parsed_text = re.sub(r\"[{}]+\".format(punctuation), '', parsed_text)\n",
        "        parsed_text = re.sub(space_pattern, ' ', parsed_text)\n",
        "        data[i] = parsed_text\n",
        "    return data\n",
        "\n",
        "tweets = preprocess(tweets)\n",
        "print(\"Sample processed tweet:\", tweets[0])\n",
        "\n",
        "# Tokenize and encode data\n",
        "input_ids = []\n",
        "attention_masks = []\n",
        "for tweet in tweets:\n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "                        tweet,\n",
        "                        add_special_tokens = True,\n",
        "                        max_length = 512,\n",
        "                        truncation = True,\n",
        "                        padding = 'max_length',\n",
        "                        return_attention_mask = True,\n",
        "                        return_tensors = 'pt',\n",
        "                   )\n",
        "\n",
        "    input_ids.append(encoded_dict['input_ids'])\n",
        "    attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "# Convert lists to tensors\n",
        "input_ids = torch.cat(input_ids, dim=0)\n",
        "attention_masks = torch.cat(attention_masks, dim=0)\n",
        "labels = torch.tensor(labels)\n",
        "\n",
        "# Create dataset and split to train/validation\n",
        "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
        "train_size = int(0.9 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size], generator=torch.Generator().manual_seed(42))\n",
        "\n",
        "print(f'{train_size:>5,} training samples')\n",
        "print(f'{val_size:>5,} validation samples')\n",
        "\n",
        "# Create DataLoaders for evaluation\n",
        "batch_size = 16\n",
        "validation_dataloader = DataLoader(\n",
        "            val_dataset,\n",
        "            shuffle = False,\n",
        "            batch_size = batch_size\n",
        "        )\n",
        "\n",
        "# Custom BERT model with frozen encoder (needed for loading)\n",
        "class BertFrozenEncoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BertFrozenEncoder, self).__init__()\n",
        "        # Load base BERT model\n",
        "        self.bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "        # Freeze all BERT parameters\n",
        "        for param in self.bert.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        # Add classifier layer\n",
        "        self.classifier = nn.Linear(768, 2)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None, token_type_ids=None, labels=None):\n",
        "        # Get BERT outputs\n",
        "        outputs = self.bert(input_ids=input_ids,\n",
        "                          attention_mask=attention_mask,\n",
        "                          token_type_ids=token_type_ids)\n",
        "\n",
        "        # Get CLS token representation (for classification)\n",
        "        pooled_output = outputs.pooler_output\n",
        "\n",
        "        # Get logits\n",
        "        logits = self.classifier(pooled_output)\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            loss_fct = nn.CrossEntropyLoss()\n",
        "            loss = loss_fct(logits, labels)\n",
        "\n",
        "        # Return outputs similar to BertForSequenceClassification\n",
        "        return type('BertOutput', (), {\n",
        "            'loss': loss,\n",
        "            'logits': logits,\n",
        "            'hidden_states': outputs.hidden_states,\n",
        "            'attentions': outputs.attentions\n",
        "        })\n",
        "\n",
        "# Extract important features\n",
        "def get_feature_importance(model, validation_dataloader, model_type=\"unfrozen\", num_samples=100):\n",
        "    \"\"\"Analyze feature importance based on attention weights\"\"\"\n",
        "    print(f\"\\nExtracting feature importance for {model_type} model...\")\n",
        "\n",
        "    # Sample data\n",
        "    sample_input_ids = []\n",
        "    sample_masks = []\n",
        "    sample_labels = []\n",
        "\n",
        "    count = 0\n",
        "    for batch in validation_dataloader:\n",
        "        if count >= num_samples:\n",
        "            break\n",
        "\n",
        "        batch_size = batch[0].size(0)\n",
        "        samples_to_take = min(batch_size, num_samples - count)\n",
        "\n",
        "        sample_input_ids.append(batch[0][:samples_to_take])\n",
        "        sample_masks.append(batch[1][:samples_to_take])\n",
        "        sample_labels.append(batch[2][:samples_to_take])\n",
        "\n",
        "        count += samples_to_take\n",
        "\n",
        "    sample_input_ids = torch.cat(sample_input_ids, dim=0).to(device)\n",
        "    sample_masks = torch.cat(sample_masks, dim=0).to(device)\n",
        "    sample_labels = torch.cat(sample_labels, dim=0).to(device)\n",
        "\n",
        "    model.eval()\n",
        "    word_importance = {}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Get model outputs with attention weights\n",
        "        if model_type == \"unfrozen\":\n",
        "            outputs = model(input_ids=sample_input_ids,\n",
        "                          attention_mask=sample_masks,\n",
        "                          labels=sample_labels,\n",
        "                          output_attentions=True)\n",
        "            attentions = outputs.attentions[-1]  # Last layer attention\n",
        "        else:\n",
        "            # For the custom model, attention may need to be accessed differently\n",
        "            outputs = model.bert(input_ids=sample_input_ids,\n",
        "                               attention_mask=sample_masks,\n",
        "                               output_attentions=True)\n",
        "            attentions = outputs.attentions[-1]\n",
        "\n",
        "    # Average attention weights across heads and samples\n",
        "    avg_attention = attentions.mean(dim=[0, 1])  # Shape: [seq_len, seq_len]\n",
        "\n",
        "    # Get attention from CLS token (used for classification)\n",
        "    cls_attention = avg_attention[0, :].cpu().numpy()\n",
        "\n",
        "    # Map tokens to attention weights\n",
        "    for i in range(min(100, len(cls_attention))):  # Look at top tokens\n",
        "        if cls_attention[i] > 0.01:  # Only consider significant attention\n",
        "            token_id = sample_input_ids[0][i].item()\n",
        "            token = tokenizer.convert_ids_to_tokens([token_id])[0]\n",
        "            if token not in word_importance:\n",
        "                word_importance[token] = cls_attention[i]\n",
        "            else:\n",
        "                word_importance[token] = max(word_importance[token], cls_attention[i])\n",
        "\n",
        "    # Sort by importance\n",
        "    sorted_words = sorted(word_importance.items(), key=lambda x: x[1], reverse=True)\n",
        "    top_words = sorted_words[:30]\n",
        "\n",
        "    print(f\"Top 30 important words for {model_type} model:\")\n",
        "    for token, importance in top_words:\n",
        "        print(f\"{token}: {importance:.4f}\")\n",
        "\n",
        "    # Create word cloud for visualization\n",
        "    wordcloud = WordCloud(width=800, height=400, background_color=\"white\",\n",
        "                        colormap=\"viridis\", max_words=100)\n",
        "    wordcloud.generate_from_frequencies(word_importance)\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "    plt.axis(\"off\")\n",
        "    plt.title(f\"Important Features - {model_type.capitalize()} BERT\")\n",
        "    plt.savefig(f\"feature_importance_{model_type}.png\")\n",
        "    plt.close()\n",
        "\n",
        "    return word_importance\n",
        "\n",
        "# Error Analysis Function\n",
        "def analyze_errors(model, dataloader, model_type=\"bert_unfrozen\"):\n",
        "    \"\"\"Analyze model errors to understand strengths and weaknesses\"\"\"\n",
        "    print(f\"\\nAnalyzing errors for {model_type} model...\")\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    all_texts = []\n",
        "    all_probs = []  # For confidence analysis\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            input_ids = batch[0].to(device)\n",
        "            input_mask = batch[1].to(device)\n",
        "            labels = batch[2].to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(input_ids=input_ids, attention_mask=input_mask)\n",
        "\n",
        "            # Get predictions\n",
        "            if model_type == \"bert_unfrozen\":\n",
        "                logits = outputs.logits\n",
        "            else:  # For custom model\n",
        "                logits = outputs.logits\n",
        "\n",
        "            probs = torch.nn.functional.softmax(logits, dim=1)\n",
        "            preds = torch.argmax(logits, dim=1)\n",
        "\n",
        "            # Convert to lists\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_probs.extend(probs.cpu().numpy())\n",
        "\n",
        "            # Get original texts\n",
        "            for i in range(input_ids.size(0)):\n",
        "                text = tokenizer.decode(input_ids[i], skip_special_tokens=True)\n",
        "                all_texts.append(text)\n",
        "\n",
        "    # Separate into correct and incorrect predictions\n",
        "    correct = []\n",
        "    incorrect = []\n",
        "    for i in range(len(all_preds)):\n",
        "        if all_preds[i] == all_labels[i]:\n",
        "            correct.append((all_texts[i], all_labels[i], all_probs[i]))\n",
        "        else:\n",
        "            incorrect.append((all_texts[i], all_labels[i], all_preds[i], all_probs[i]))\n",
        "\n",
        "    # Analysis of errors\n",
        "    print(f\"Total examples: {len(all_preds)}\")\n",
        "    print(f\"Correct predictions: {len(correct)} ({len(correct)/len(all_preds)*100:.2f}%)\")\n",
        "    print(f\"Incorrect predictions: {len(incorrect)} ({len(incorrect)/len(all_preds)*100:.2f}%)\")\n",
        "\n",
        "    # Confusion matrix\n",
        "    cm = confusion_matrix(all_labels, all_preds)\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=['Fake', 'Real'],\n",
        "                yticklabels=['Fake', 'Real'])\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        "    plt.title(f'Confusion Matrix - {model_type}')\n",
        "    plt.savefig(f'confusion_matrix_{model_type}.png')\n",
        "    plt.close()\n",
        "\n",
        "    # Detailed classification report\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(all_labels, all_preds, target_names=['Fake', 'Real']))\n",
        "\n",
        "    # Analyze error patterns\n",
        "    real_as_fake = [item for item in incorrect if item[1] == 1 and item[2] == 0]\n",
        "    fake_as_real = [item for item in incorrect if item[1] == 0 and item[2] == 1]\n",
        "\n",
        "    # Length analysis\n",
        "    real_as_fake_lengths = [len(item[0].split()) for item in real_as_fake]\n",
        "    fake_as_real_lengths = [len(item[0].split()) for item in fake_as_real]\n",
        "\n",
        "    print(f\"\\nReal news classified as fake - avg length: {np.mean(real_as_fake_lengths):.2f} words\")\n",
        "    print(f\"Fake news classified as real - avg length: {np.mean(fake_as_real_lengths):.2f} words\")\n",
        "\n",
        "    # Word frequency in errors\n",
        "    def get_top_words(texts, n=20):\n",
        "        words = []\n",
        "        for text in texts:\n",
        "            words.extend(text.lower().split())\n",
        "        return Counter(words).most_common(n)\n",
        "\n",
        "    print(\"\\nMost common words in real news classified as fake:\")\n",
        "    real_as_fake_texts = [item[0] for item in real_as_fake]\n",
        "    print(get_top_words(real_as_fake_texts))\n",
        "\n",
        "    print(\"\\nMost common words in fake news classified as real:\")\n",
        "    fake_as_real_texts = [item[0] for item in fake_as_real]\n",
        "    print(get_top_words(fake_as_real_texts))\n",
        "\n",
        "    # Confidence analysis\n",
        "    real_as_fake_conf = [item[3][0] for item in real_as_fake]  # Confidence for fake class\n",
        "    fake_as_real_conf = [item[3][1] for item in fake_as_real]  # Confidence for real class\n",
        "\n",
        "    print(f\"\\nAvg confidence when real news classified as fake: {np.mean(real_as_fake_conf):.4f}\")\n",
        "    print(f\"Avg confidence when fake news classified as real: {np.mean(fake_as_real_conf):.4f}\")\n",
        "\n",
        "    # Generate word clouds for misclassified examples\n",
        "    if real_as_fake_texts:\n",
        "        wordcloud = WordCloud(width=800, height=400, background_color=\"white\",\n",
        "                            colormap=\"Reds\", max_words=100, collocations=False)\n",
        "        wordcloud.generate(\" \".join(real_as_fake_texts))\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "        plt.axis(\"off\")\n",
        "        plt.title(f\"Words in Real News Classified as Fake - {model_type}\")\n",
        "        plt.savefig(f\"real_as_fake_wordcloud_{model_type}.png\")\n",
        "        plt.close()\n",
        "\n",
        "    if fake_as_real_texts:\n",
        "        wordcloud = WordCloud(width=800, height=400, background_color=\"white\",\n",
        "                            colormap=\"Blues\", max_words=100, collocations=False)\n",
        "        wordcloud.generate(\" \".join(fake_as_real_texts))\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "        plt.axis(\"off\")\n",
        "        plt.title(f\"Words in Fake News Classified as Real - {model_type}\")\n",
        "        plt.savefig(f\"fake_as_real_wordcloud_{model_type}.png\")\n",
        "        plt.close()\n",
        "\n",
        "    # Return error examples for further analysis\n",
        "    return real_as_fake, fake_as_real, {\n",
        "        'predictions': all_preds,\n",
        "        'true_labels': all_labels,\n",
        "        'texts': all_texts,\n",
        "        'accuracy': len(correct) / len(all_preds),\n",
        "        'f1': f1_score(all_labels, all_preds)\n",
        "    }\n",
        "\n",
        "# Analyze example confidence\n",
        "def analyze_confidence(model, dataloader, model_type=\"unfrozen\"):\n",
        "    \"\"\"Analyze prediction confidence patterns\"\"\"\n",
        "    print(f\"\\nAnalyzing prediction confidence for {model_type} model...\")\n",
        "    model.eval()\n",
        "    confidences = []\n",
        "    labels = []\n",
        "    correct_pred_conf = []\n",
        "    incorrect_pred_conf = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            input_ids = batch[0].to(device)\n",
        "            input_mask = batch[1].to(device)\n",
        "            batch_labels = batch[2].to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(input_ids=input_ids, attention_mask=input_mask)\n",
        "\n",
        "            # Get predictions\n",
        "            if model_type == \"bert_unfrozen\":\n",
        "                logits = outputs.logits\n",
        "            else:  # For custom model\n",
        "                logits = outputs.logits\n",
        "\n",
        "            probs = torch.nn.functional.softmax(logits, dim=1)\n",
        "            preds = torch.argmax(logits, dim=1)\n",
        "\n",
        "            # Get confidence scores (probability of predicted class)\n",
        "            batch_confidences = torch.gather(probs, 1, preds.unsqueeze(1)).squeeze(1)\n",
        "\n",
        "            # Separate confidences for correct and incorrect predictions\n",
        "            correct_mask = (preds == batch_labels)\n",
        "\n",
        "            correct_conf = batch_confidences[correct_mask].cpu().numpy()\n",
        "            incorrect_conf = batch_confidences[~correct_mask].cpu().numpy()\n",
        "\n",
        "            correct_pred_conf.extend(correct_conf)\n",
        "            incorrect_pred_conf.extend(incorrect_conf)\n",
        "\n",
        "            confidences.extend(batch_confidences.cpu().numpy())\n",
        "            labels.extend(batch_labels.cpu().numpy())\n",
        "\n",
        "    # Confidence statistics\n",
        "    avg_confidence = np.mean(confidences)\n",
        "    avg_correct_conf = np.mean(correct_pred_conf) if correct_pred_conf else 0\n",
        "    avg_incorrect_conf = np.mean(incorrect_pred_conf) if incorrect_pred_conf else 0\n",
        "\n",
        "    print(f\"Average confidence: {avg_confidence:.4f}\")\n",
        "    print(f\"Average confidence for correct predictions: {avg_correct_conf:.4f}\")\n",
        "    print(f\"Average confidence for incorrect predictions: {avg_incorrect_conf:.4f}\")\n",
        "\n",
        "    # Plot confidence distributions\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.hist(correct_pred_conf, alpha=0.5, bins=20, label='Correct predictions')\n",
        "    plt.hist(incorrect_pred_conf, alpha=0.5, bins=20, label='Incorrect predictions')\n",
        "    plt.xlabel('Confidence')\n",
        "    plt.ylabel('Count')\n",
        "    plt.title(f'Confidence Distribution - {model_type.capitalize()} BERT')\n",
        "    plt.legend()\n",
        "    plt.savefig(f'confidence_dist_{model_type}.png')\n",
        "    plt.close()\n",
        "\n",
        "    return {\n",
        "        'avg_confidence': avg_confidence,\n",
        "        'avg_correct_conf': avg_correct_conf,\n",
        "        'avg_incorrect_conf': avg_incorrect_conf,\n",
        "        'correct_conf': correct_pred_conf,\n",
        "        'incorrect_conf': incorrect_pred_conf\n",
        "    }\n",
        "\n",
        "# Compare frozen vs unfrozen models\n",
        "def compare_models(frozen_results, unfrozen_results):\n",
        "    \"\"\"Compare performance between frozen and unfrozen BERT models\"\"\"\n",
        "    # Extract predictions and labels\n",
        "    frozen_preds = frozen_results['predictions']\n",
        "    unfrozen_preds = unfrozen_results['predictions']\n",
        "    true_labels = frozen_results['true_labels']  # Same for both\n",
        "\n",
        "    # Calculate agreement statistics\n",
        "    agreements = sum(1 for f, u in zip(frozen_preds, unfrozen_preds) if f == u)\n",
        "    agreement_rate = agreements / len(frozen_preds)\n",
        "\n",
        "    # Calculate correct agreements/disagreements\n",
        "    correct_both = sum(1 for f, u, t in zip(frozen_preds, unfrozen_preds, true_labels)\n",
        "                       if f == u and f == t)\n",
        "    wrong_both = sum(1 for f, u, t in zip(frozen_preds, unfrozen_preds, true_labels)\n",
        "                     if f == u and f != t)\n",
        "    frozen_correct_only = sum(1 for f, u, t in zip(frozen_preds, unfrozen_preds, true_labels)\n",
        "                             if f != u and f == t)\n",
        "    unfrozen_correct_only = sum(1 for f, u, t in zip(frozen_preds, unfrozen_preds, true_labels)\n",
        "                               if f != u and u == t)\n",
        "\n",
        "    # Print comparison results\n",
        "    print(\"\\nModel Comparison - Frozen vs. Unfrozen BERT:\")\n",
        "    print(f\"Agreement rate: {agreement_rate:.4f} ({agreements}/{len(frozen_preds)})\")\n",
        "    print(f\"Both correct: {correct_both}/{len(frozen_preds)} ({correct_both/len(frozen_preds)*100:.2f}%)\")\n",
        "    print(f\"Both wrong: {wrong_both}/{len(frozen_preds)} ({wrong_both/len(frozen_preds)*100:.2f}%)\")\n",
        "    print(f\"Only frozen correct: {frozen_correct_only}/{len(frozen_preds)} ({frozen_correct_only/len(frozen_preds)*100:.2f}%)\")\n",
        "    print(f\"Only unfrozen correct: {unfrozen_correct_only}/{len(frozen_preds)} ({unfrozen_correct_only/len(frozen_preds)*100:.2f}%)\")\n",
        "\n",
        "    # Accuracy comparison\n",
        "    print(f\"\\nFrozen model accuracy: {frozen_results['accuracy']:.4f}\")\n",
        "    print(f\"Unfrozen model accuracy: {unfrozen_results['accuracy']:.4f}\")\n",
        "\n",
        "    # F1 score comparison\n",
        "    print(f\"Frozen model F1 score: {frozen_results['f1']:.4f}\")\n",
        "    print(f\"Unfrozen model F1 score: {unfrozen_results['f1']:.4f}\")\n",
        "\n",
        "    # Create comparison plot\n",
        "    metrics = ['Accuracy', 'F1 Score']\n",
        "    frozen_values = [frozen_results['accuracy'], frozen_results['f1']]\n",
        "    unfrozen_values = [unfrozen_results['accuracy'], unfrozen_results['f1']]\n",
        "\n",
        "    x = np.arange(len(metrics))\n",
        "    width = 0.35\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(10, 6))\n",
        "    frozen_bars = ax.bar(x - width/2, frozen_values, width, label='Frozen BERT')\n",
        "    unfrozen_bars = ax.bar(x + width/2, unfrozen_values, width, label='Unfrozen BERT')\n",
        "\n",
        "    ax.set_ylabel('Score')\n",
        "    ax.set_title('Performance Comparison: Frozen vs. Unfrozen BERT')\n",
        "    ax.set_xticks(x)\n",
        "    ax.set_xticklabels(metrics)\n",
        "    ax.legend()\n",
        "\n",
        "    # Add values on top of bars\n",
        "    def add_labels(bars):\n",
        "        for bar in bars:\n",
        "            height = bar.get_height()\n",
        "            ax.annotate(f'{height:.4f}',\n",
        "                        xy=(bar.get_x() + bar.get_width() / 2, height),\n",
        "                        xytext=(0, 3),  # 3 points vertical offset\n",
        "                        textcoords=\"offset points\",\n",
        "                        ha='center', va='bottom')\n",
        "\n",
        "    add_labels(frozen_bars)\n",
        "    add_labels(unfrozen_bars)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('model_comparison.png')\n",
        "    plt.close()\n",
        "\n",
        "    # Analyze examples where models disagree\n",
        "    disagreements = []\n",
        "    for i in range(len(frozen_preds)):\n",
        "        if frozen_preds[i] != unfrozen_preds[i]:\n",
        "            disagreements.append({\n",
        "                'text': frozen_results['texts'][i],\n",
        "                'true_label': true_labels[i],\n",
        "                'frozen_pred': frozen_preds[i],\n",
        "                'unfrozen_pred': unfrozen_preds[i]\n",
        "            })\n",
        "\n",
        "    # Print some examples where models disagree\n",
        "    print(\"\\nExamples where models disagree:\")\n",
        "    for i, example in enumerate(disagreements[:5]):\n",
        "        print(f\"\\nExample {i+1}:\")\n",
        "        print(f\"Text: {example['text'][:100]}...\")\n",
        "        print(f\"True label: {'Real' if example['true_label'] == 1 else 'Fake'}\")\n",
        "        print(f\"Frozen BERT prediction: {'Real' if example['frozen_pred'] == 1 else 'Fake'}\")\n",
        "        print(f\"Unfrozen BERT prediction: {'Real' if example['unfrozen_pred'] == 1 else 'Fake'}\")\n",
        "        print(f\"Correct model: {'Both' if (example['frozen_pred'] == example['true_label'] and example['unfrozen_pred'] == example['true_label']) else 'Frozen' if example['frozen_pred'] == example['true_label'] else 'Unfrozen' if example['unfrozen_pred'] == example['true_label'] else 'Neither'}\")\n",
        "\n",
        "    # Save the disagreement examples\n",
        "    with open('model_disagreements.txt', 'w') as f:\n",
        "        f.write(f\"Total disagreements: {len(disagreements)}\\n\\n\")\n",
        "        for i, example in enumerate(disagreements):\n",
        "            f.write(f\"Example {i+1}:\\n\")\n",
        "            f.write(f\"Text: {example['text']}\\n\")\n",
        "            f.write(f\"True label: {'Real' if example['true_label'] == 1 else 'Fake'}\\n\")\n",
        "            f.write(f\"Frozen BERT prediction: {'Real' if example['frozen_pred'] == 1 else 'Fake'}\\n\")\n",
        "            f.write(f\"Unfrozen BERT prediction: {'Real' if example['unfrozen_pred'] == 1 else 'Fake'}\\n\")\n",
        "            f.write(f\"Correct model: {'Both' if (example['frozen_pred'] == example['true_label'] and example['unfrozen_pred'] == example['true_label']) else 'Frozen' if example['frozen_pred'] == example['true_label'] else 'Unfrozen' if example['unfrozen_pred'] == example['true_label'] else 'Neither'}\\n\\n\")\n",
        "\n",
        "    return {\n",
        "        'agreement_rate': agreement_rate,\n",
        "        'correct_both': correct_both / len(frozen_preds),\n",
        "        'wrong_both': wrong_both / len(frozen_preds),\n",
        "        'frozen_correct_only': frozen_correct_only / len(frozen_preds),\n",
        "        'unfrozen_correct_only': unfrozen_correct_only / len(frozen_preds),\n",
        "        'disagreements': disagreements\n",
        "    }\n",
        "\n",
        "# Main function for loading and analyzing models\n",
        "def analyze_trained_models():\n",
        "    print(\"Loading trained models...\")\n",
        "\n",
        "    # Load unfrozen model\n",
        "    unfrozen_model = BertForSequenceClassification.from_pretrained(\n",
        "        \"bert-base-uncased\",\n",
        "        num_labels = 2,\n",
        "    )\n",
        "    try:\n",
        "        # Try to load with state_dict\n",
        "        unfrozen_model.load_state_dict(torch.load(\"bert_unfrozen.pt\", map_location='cpu'))\n",
        "        print(\"Loaded unfrozen model state_dict successfully\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading unfrozen model state_dict: {e}\")\n",
        "        print(\"This may be because your model was saved in a different format.\")\n",
        "        try:\n",
        "            # Try to load full model with weights_only=False\n",
        "            unfrozen_model = torch.load(\"bert_unfrozen.pt\", map_location='cpu', weights_only=False)\n",
        "            print(\"Loaded unfrozen model successfully with weights_only=False\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading unfrozen model: {e}\")\n",
        "            print(\"Using a newly initialized model instead - results will not be accurate!\")\n",
        "\n",
        "    unfrozen_model.to(device)\n",
        "    unfrozen_model.eval()\n",
        "\n",
        "    # Load frozen model\n",
        "    frozen_model = BertFrozenEncoder()\n",
        "    try:\n",
        "        # Try to load with state_dict\n",
        "        frozen_model.load_state_dict(torch.load(\"bert_frozen.pt\", map_location='cpu'))\n",
        "        print(\"Loaded frozen model state_dict successfully\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading frozen model state_dict: {e}\")\n",
        "        print(\"This may be because your model was saved in a different format.\")\n",
        "        try:\n",
        "            # Try to load full model with weights_only=False\n",
        "            frozen_model = torch.load(\"bert_frozen.pt\", map_location='cpu', weights_only=False)\n",
        "            print(\"Loaded frozen model successfully with weights_only=False\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading frozen model: {e}\")\n",
        "            print(\"Using a newly initialized model instead - results will not be accurate!\")\n",
        "\n",
        "    frozen_model.to(device)\n",
        "    frozen_model.eval()\n",
        "\n",
        "    # Analyze feature importance\n",
        "    try:\n",
        "        unfrozen_features = get_feature_importance(unfrozen_model, validation_dataloader, \"unfrozen\")\n",
        "        frozen_features = get_feature_importance(frozen_model, validation_dataloader, \"frozen\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error during feature importance analysis: {e}\")\n",
        "        print(\"Skipping feature importance analysis.\")\n",
        "\n",
        "    # Analyze confidence patterns\n",
        "    try:\n",
        "        unfrozen_conf = analyze_confidence(unfrozen_model, validation_dataloader, \"unfrozen\")\n",
        "        frozen_conf = analyze_confidence(frozen_model, validation_dataloader, \"frozen\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error during confidence analysis: {e}\")\n",
        "        print(\"Skipping confidence analysis.\")\n",
        "\n",
        "    # Analyze errors\n",
        "    print(\"Analyzing unfrozen model errors...\")\n",
        "    _, _, unfrozen_results = analyze_errors(unfrozen_model, validation_dataloader, \"bert_unfrozen\")\n",
        "\n",
        "    print(\"Analyzing frozen model errors...\")\n",
        "    _, _, frozen_results = analyze_errors(frozen_model, validation_dataloader, \"bert_frozen\")\n",
        "\n",
        "    # Compare models\n",
        "    print(\"Comparing models...\")\n",
        "    comparison_results = compare_models(frozen_results, unfrozen_results)\n",
        "\n",
        "    # Combine all results into a summary report\n",
        "    with open('analysis_summary.txt', 'w') as f:\n",
        "        f.write(\"ANALYSIS SUMMARY\\n\")\n",
        "        f.write(\"===============\\n\\n\")\n",
        "\n",
        "        f.write(\"MODEL PERFORMANCE\\n\")\n",
        "        f.write(\"----------------\\n\")\n",
        "        f.write(f\"Frozen BERT accuracy: {frozen_results['accuracy']:.4f}\\n\")\n",
        "        f.write(f\"Unfrozen BERT accuracy: {unfrozen_results['accuracy']:.4f}\\n\")\n",
        "        f.write(f\"Frozen BERT F1 score: {frozen_results['f1']:.4f}\\n\")\n",
        "        f.write(f\"Unfrozen BERT F1 score: {unfrozen_results['f1']:.4f}\\n\\n\")\n",
        "\n",
        "        f.write(\"MODEL AGREEMENT ANALYSIS\\n\")\n",
        "        f.write(\"-----------------------\\n\")\n",
        "        f.write(f\"Agreement rate: {comparison_results['agreement_rate']:.4f}\\n\")\n",
        "        f.write(f\"Both correct: {comparison_results['correct_both']*100:.2f}%\\n\")\n",
        "        f.write(f\"Both wrong: {comparison_results['wrong_both']*100:.2f}%\\n\")\n",
        "        f.write(f\"Only frozen correct: {comparison_results['frozen_correct_only']*100:.2f}%\\n\")\n",
        "        f.write(f\"Only unfrozen correct: {comparison_results['unfrozen_correct_only']*100:.2f}%\\n\\n\")\n",
        "\n",
        "        try:\n",
        "            f.write(\"CONFIDENCE ANALYSIS\\n\")\n",
        "            f.write(\"------------------\\n\")\n",
        "            f.write(f\"Frozen model average confidence: {frozen_conf['avg_confidence']:.4f}\\n\")\n",
        "            f.write(f\"Unfrozen model average confidence: {unfrozen_conf['avg_confidence']:.4f}\\n\")\n",
        "            f.write(f\"Frozen model confidence on correct predictions: {frozen_conf['avg_correct_conf']:.4f}\\n\")\n",
        "            f.write(f\"Unfrozen model confidence on correct predictions: {unfrozen_conf['avg_correct_conf']:.4f}\\n\")\n",
        "            f.write(f\"Frozen model confidence on incorrect predictions: {frozen_conf['avg_incorrect_conf']:.4f}\\n\")\n",
        "            f.write(f\"Unfrozen model confidence on incorrect predictions: {unfrozen_conf['avg_incorrect_conf']:.4f}\\n\\n\")\n",
        "        except:\n",
        "            f.write(\"Confidence analysis not available.\\n\\n\")\n",
        "\n",
        "        f.write(\"CONCLUSIONS\\n\")\n",
        "        f.write(\"-----------\\n\")\n",
        "        if frozen_results['accuracy'] > unfrozen_results['accuracy']:\n",
        "            f.write(\"The frozen BERT model performed better in terms of accuracy. This suggests that for this COVID-19 fake news detection task, the pre-trained BERT features were sufficient, and fine-tuning the entire model might have led to overfitting.\\n\\n\")\n",
        "        else:\n",
        "            f.write(\"The unfrozen BERT model performed better in terms of accuracy. This suggests that for this COVID-19 fake news detection task, fine-tuning the entire model helped to adapt the features to the specific domain.\\n\\n\")\n",
        "\n",
        "        if comparison_results['frozen_correct_only'] > comparison_results['unfrozen_correct_only']:\n",
        "            f.write(\"The frozen model correctly classified more examples that the unfrozen model missed. This suggests the frozen model might be more robust to certain types of examples.\\n\\n\")\n",
        "        else:\n",
        "            f.write(\"The unfrozen model correctly classified more examples that the frozen model missed. This suggests the fine-tuning process helped capture patterns that the frozen model couldn't detect.\\n\\n\")\n",
        "\n",
        "    print(\"\\nAnalysis complete. Results saved to files.\")\n",
        "\n",
        "    return unfrozen_model, frozen_model, unfrozen_results, frozen_results, comparison_results\n",
        "\n",
        "# Run the analysis\n",
        "if __name__ == \"__main__\":\n",
        "    # Install required packages if running on Colab\n",
        "    try:\n",
        "        import google.colab\n",
        "        print(\"Running on Colab, installing required packages...\")\n",
        "        !pip install -q wordcloud seaborn transformers\n",
        "    except:\n",
        "        print(\"Not running on Colab\")\n",
        "\n",
        "    # Run the analysis function\n",
        "    unfrozen_model, frozen_model, unfrozen_results, frozen_results, comparison_results = analyze_trained_models()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "u1oft8AGgFGL",
      "metadata": {
        "id": "u1oft8AGgFGL"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
